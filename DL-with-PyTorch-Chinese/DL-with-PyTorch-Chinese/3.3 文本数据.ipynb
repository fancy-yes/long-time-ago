{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习席卷了自然语言处理（natural language processing, NLP）领域，尤其是通过使用不断消耗输入和模型先前输出相结合的模型。这种模型称为递归神经网络（recurrent neural networks, RNN），它已被成功应用于文本分类、文本生成和自动翻译系统。在这之前的NLP工作的特点是复杂的多阶段处理流程，包括编码语言语法的规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本章中，你的目标是将文本转换成神经网络可以处理的东西，就像前面的例子一样，即数值张量。在处理成数值张量之后，再为你的文本处理工作选择正确的网络结构，然后就可以使用PyTorch进行NLP了。你马上就会看到此功能的强大之处：如果你以正确的形式提出了问题，就可以使用相同的PyTorch工具在不同领域中的任务上达到目前最先进的性能。这项工作的第一部分是重塑数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网络在两个级别上对文本进行操作：在字符级别上，一次处理一个字符；而在单词级别上，单词是网络中最细粒度的实体。无论是在字符级别还是在单词级别操作，将文本信息编码为张量形式的技术都是相同的。这种技术没什么神奇的，你之前已经用过了，即独热编码。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 开始"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载Jane Austen的《傲慢与偏见》。保存文件并读入文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_path = r\".\\data\\chapter3\\1342-0.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.loadtxt(dset_path,encoding='utf-8')   # 错误: 未知原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=dset_path,mode='r',encoding='utf-8') as f:    # 此处gbk格式读取将出错\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对文本中的字符操作，并为每个字符进行独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'“Impossible, Mr. Bennet, impossible, when I am not acquainted with him'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = text.split('\\n') #等价于text.splitlines()\n",
    "line = lines[200]\n",
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(97, 98)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 做一个测试\n",
    "ord('a'),ord('b') #将转为ascii对应的编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 128])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个零张量，其每一行都要能容纳独热编码字符 \n",
    "    #备注： 按照 ASCII 有 128个字符   \n",
    "        # 所以每个字符的独热编码有128位 (抛弃ASCII未编码的字符)\n",
    "letter_tensor = torch.zeros(len(line),128)  \n",
    "\n",
    "\n",
    "# 为上面的零张量在正确的位置上设置 1\n",
    "for i,letter in enumerate(line.lower().strip()):\n",
    "    letter_index = ord(letter) if ord(letter) < 128 else 0\n",
    "    letter_tensor[i][letter_index] = 1\n",
    "    \n",
    "letter_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过建立词汇表来在词级别（word-level）对句子（即词序列）进行独热编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('123', '12？3')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "    #strip()只能去除两边的指定符号，中间的不可以去除\n",
    "\"123？\".strip('？'),\"12？3\".strip('？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义clean_words函数，其接受文本，并返回小写，并删除无关符号，比如标点。\n",
    "def clean_words(input_str):\n",
    "    word_list = input_str.lower().replace('\\n',' ').split()\n",
    "    word_list = [word.strip('.,;:\"!?”“_-')  for word in word_list]\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('“Impossible, Mr. Bennet, impossible, when I am not acquainted with him',\n",
       " ['impossible',\n",
       "  'mr',\n",
       "  'bennet',\n",
       "  'impossible',\n",
       "  'when',\n",
       "  'i',\n",
       "  'am',\n",
       "  'not',\n",
       "  'acquainted',\n",
       "  'with',\n",
       "  'him'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_line = clean_words(line)\n",
    "\n",
    "line,words_in_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 0, 'like': 1, 'you': 2}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 做一个测试\n",
    "    # 枚举\n",
    "{ word:i for (i,word) in enumerate(['i','like','you',])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 经过以上的对一行文本的分析。\n",
    "现在用同样的方式来对整个文本操作，得到整个文本的单词列表，并且为这些单词建立索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'a', 'd', 'e'}, ['a', 'd', 'e'])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试\n",
    "    # set()转化为集合 \n",
    "        #sort 与 sorted 区别：sort 是应用在 list 上的方法，sorted 可以对所有可迭代的对象进行排序操作。\n",
    "set(['a','e','d']),sorted(set(['a','e','d']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7261, 2940)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = sorted(set(clean_words(text)))\n",
    "word2index_dict = {word:i for i,word in enumerate(word_list)}\n",
    "\n",
    "# word2index_dict是一个字典，其中单词作为键，而整数作为值。\n",
    "    # 独热编码时，你将使用此词典来有效地找到单词的索引\n",
    "len(word2index_dict),word2index_dict['good']  # 显示有7261个单词，单词good的索引是2940"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 继续对一个句子中的单词(词序列)操作，进行独热编码\n",
    "（再早之前操作是正对字符进行操作，包括对字符的独热编码）\n",
    "现在对单词进行独热编码，独热编码的位数将不再是128位，而是 len(word2index_dict) 位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 单词:impossible   词索引: 3394 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  1 单词:mr           词索引: 4305 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  2 单词:bennet       词索引:  813 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  3 单词:impossible   词索引: 3394 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  4 单词:when         词索引: 7078 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  5 单词:i            词索引: 3315 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  6 单词:am           词索引:  415 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  7 单词:not          词索引: 4436 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  8 单词:acquainted   词索引:  239 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      "  9 单词:with         词索引: 7148 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n",
      " 10 单词:him          词索引: 3215 独热:tensor([0., 0., 0.,  ..., 0., 0., 0.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([70, 7261])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在是对文本中任意句子的分析。将为任意句子中的单词进行独热编码。\n",
    "    # 建立零张量，该张量的行应该能容纳 len(word2index_dict) 位的独热编码。\n",
    "\n",
    "word_tensor = torch.zeros(len(line),len(word2index_dict))\n",
    "for i,word in enumerate(words_in_line):\n",
    "    word_index = word2index_dict[word]\n",
    "    word_tensor[i][word_index] = 1\n",
    "    print( \"{:3} 单词:{:12} 词索引:{:5} 独热:{}\".format(i,word,word_index,word_tensor[i]))\n",
    "    \n",
    "    \n",
    "word_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3.1.1 文本嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "独热编码是一种将类别数据表示成张量的很有用技术。就像你可能预料到的那样，当需要编码的项目数很大（例如语料库中的单词）时，独热编码就开始崩溃了。一本书中有超过7000个单词！\n",
    "\n",
    "当然，你可以做一些工作来对单词进行去重、压缩替代拼写、将过去和将来时统一为相同表示，等等。尽管如此，通用的英文编码仍将是巨大的。更糟糕的是，每次遇到一个新单词时，都必须在向量中添加一个新列，这意味着要在模型中添加一组新的权重以解决该新词汇输入问题，从训练角度看这将给你带来很大的痛苦。\n",
    "\n",
    "如何将编码压缩为更易于管理的大小，并限制大小增长？好吧，可以使用浮点数向量，而不是使用多个0和一个1的向量。举例来说，一个含100个浮点数的向量就可以表示很大量的词汇。**关键是找到一种有效的方法，以一种有助于下游学习的方式将单个单词映射到这个100维空间。这种技术称为嵌入（embedding）**。\n",
    "\n",
    "原则上，你可以遍历词汇表并为每个单词生成100个随机浮点数。 这种方法可能是有效的，因为你可以将大量词汇塞入100个数字中，但是它会丢弃掉基于语义或上下文的单词之间的任何距离信息。使用这种词嵌入的模型不得不处理其输入向量中的少量结构。**理想的解决方案是以这样的方式生成嵌入：用于同一上下文的单词映射到嵌入空间的邻近区域。**\n",
    "\n",
    "如果要手工设计解决此问题的方法，你有可能决定通过沿轴映射基本名词和形容词来构建嵌入空间。你可以生成一个二维空间，在该空间中，两个坐标轴分别映射到名词“水果”（0.0-0.33）、“花”（0.33-0.66）和“狗”（0.66-1.0），以及形容词“红色”（0.0-0.2）、“橙色”（0.2-0.4）、“黄色”（0.4-0.6）、“白色”（0.6-0.8）和“棕色”（0.8-1.0）。你现在的目标是将水果、花和狗放置在嵌入中。\n",
    "\n",
    "开始嵌入单词时，可以将“苹果”映射到“水果”和“红色”象限中的某个数。同样，你可以轻松地映射“橘子”、“柠檬”、“荔枝”和“猕猴桃”（五颜六色的水果）。然后，你可以从花开始，分配“玫瑰”、“罂粟”、“水仙花”、“百合”和...好吧，不存在很多棕色的花。好，“太阳花”可以推出“花”、“黄色”和“棕色”，而“雏菊”可以推出“花”、“白色”和“黄色”。也许你应该更新“猕猴桃”以将其映射到“水果”、“棕色”和“绿色”附近。对于狗和颜色，“redbone（译者注：狗的品种）”、“fox”可能是“橙色”、“金毛”和“贵宾犬”可是“白色”的，以及...大多数种类的狗都是“棕色”的。\n",
    "\n",
    "尽管对于大型语料库而言，手动进行此映射并不可行，但你应注意，尽管嵌入大小仅为2，但你描述了除基数8个之外的15个不同的单词，如果你花一些创造性的时间，可能还会嵌入更多的单词。\n",
    "\n",
    "你可能已经猜到了，这种工作是可以自动进行的。**通过处理大量文本语料库，你可以生成与此类似的嵌入。 主要区别在于嵌入向量具有100到1000个元素，并且坐标轴不直接映射到某个词义，但是意思相近的词映射到嵌入空间也是相近的，其轴可能是任意的浮点维（floating-point dimensions）。**\n",
    "尽管实际使用的算法（比如word2vec）对于我们在此要关注的内容来说有点超出范围，但值得一提的是，**嵌入通常是使用神经网络并试图根据句中邻近词（上下文）预测某个词而生成的。在这种情况下，你可以从独热编码的单词开始，使用（通常是相当浅的）神经网络来生成嵌入。当嵌入可用时，你就可以将其用于下游任务。**\n",
    "\n",
    "**生成的嵌入的一个有趣的方面是，相似的词不仅会聚在一起，还会与其他词保持一致的空间关系。如果你要使用“苹果”的嵌入向量，并加上和减去其他词的嵌入向量，就可以进行类比，例如苹果 - 红色 - 甜 + 酸，最后可能得到一个类似“柠檬”的向量。**\n",
    "\n",
    "我们不会在这里使用文本嵌入，但是当必须用数字向量表示集合中的大量元素时，它们是必不可少的工具。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
